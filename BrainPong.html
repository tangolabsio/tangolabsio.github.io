<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta property="og:title" content="Tango Labs" />
    <title>BrainPong</title>
    <meta property="og:description" content="Tango Labs" />
    <meta property="og:site_name" content="Tango Labs" />
    <meta property="og:url" content="http://tangolabs.io/" />
    <meta property="og:type" content="website" />
    <meta
      name="description"
      content="BrainPong is a traditional pong game that is controlled by one's brain."
    />

    <link href="blog-post.css" rel="stylesheet" />
    <link rel="icon" type="image/png" href="./tangolabs.png" />
  </head>

  <body>
    <div id="page-container">
      <div id="top">
        <h1>BrainPong</h1>
        <p>James Wu, Nikolas Ioannou, Sage Khanuja</p>
        <p>December 5, 2019</p>
        <br />
        <br />
        <br />
        <!-- <img src="BrainPong-header.png" alt="BrainPong">
      <br>
      <br>
      <br>
      <br> -->
      </div>

      <p><b>Description</b></p>
      <p>
        BrainPong is a traditional pong game that is controlled by one's brain.
      </p>
      <br />

      <p><b>Frontend</b></p>
      <p>
        We developed a pong game in Python with PyGame. The pong game consists
        of three gamemodes:
      </p>
      <p>1) Brain vs. person</p>
      <p>2) Brain vs. computer</p>
      <p>3) Brain vs. brain (requires two BCIs)</p>
      <br />

      <p><b>Data collection</b></p>
      <p>
        We built and implemented OpenBCIâ€™s Ultracortex Mark IV with a Cyton
        biosensing board and the Muse 2 as a means of reading EEG data from the brain. We
        recorded EEG data from 16 channels at 60Hz (gamma waves) of the wearer
        thinking left and right. Recognizing the importance of capturing
        sequential data, we made each data point include 60 frames of EEG data.
        In total, we collected 10,000 data points across three wearers. While
        collecting data, we ensured that the wearer did not contract many facial
        muscles to reduce noise in the EEG data.
      </p>
      <br />

      <p><b>Machine learning</b></p>
      <p>
        Using the EEG data that we collected, we trained a CNN to classify it as
        left and right. After training, we received a 92% validation accuracy,
        with 1,000 data points for each class in the validation dataset.
        Finally, we used the output of this model to move the paddle
        accordingly.
      </p>

      <!-- BRain pong at grey natters -->

      <div id="top">
        <br />
        <br />
        <br />
      </div>
    </div>
  </body>
</html>
